+++
title = "Housemaker"
date = 2024-08-03
+++


"King is to Queen, what man is to woman." This example is often used in Natural Language Processing papers to show that language models are capable of forming a semantic space which allows them to "understand" the world. But biases exist in all of these language models: the biases are cultural, racists, sexists, you name it. Today at a conference I atend, someone showed that according to GPT, it is also true to say that "housemaker" is to "programmer" what woman is to man. Nothing surprising: even before the advent of LLMs and ChatGPT, NLP specialists have been studying how biased AI models are. Today, the spontaneous "oh!" that collectively escaped the lips of the audience shows that most of them didn't think these biases could be so blatant.
